# CS5760 â€“ Natural Language Processing
## Homework 2

**Student Name:** Shaik Karishma  
**Student ID:** 700768890



---

# Part I â€” Writing / Calculations 

## Q1) Worked Example: Classify â€œpredictable no funâ€
Naive Bayes score:

Score(c) = P(c) Ã— Î  P(w_i | c)

For **â€œpredictable no funâ€**:
- Score(pos) = P(pos)Â·P(predictable|pos)Â·P(no|pos)Â·P(fun|pos)
- Score(neg) = P(neg)Â·P(predictable|neg)Â·P(no|neg)Â·P(fun|neg)

 Pick the class with the larger score. (Notebook computes both scores once you plug in the given likelihoods from the slide/Q2.)

#Q2) Harms of Classification

(a) Representational harm
Representational harm happens when a system reinforces stereotypes or unfair associations about a group through how it labels/represents them. In the Kiritchenko & Mohammad (2018) type of setup, this harm is shown when model outputs (e.g., emotion/association predictions) systematically differ across demographic identity terms, reflecting biased language patterns in training data and causing unfair representation.

(b) One risk of censorship in toxicity classification
A key risk is over-blocking or silencing: systems may label non-toxic content as toxic (especially when it contains identity terms or reclaimed language), causing communities to get unfairly moderated and reducing legitimate speech.

(c) Why worse on African American English / Indian English
Models often perform worse because training/test data is not balanced across dialects. If a model is mostly trained on â€œstandardâ€ English, dialect grammar/spelling/phrasing looks â€œunusualâ€ to the model, increasing false positives/negatives.


## Q3) Bigram Probabilities + Zero Probability
### (A) Sentence probabilities (MLE)
Bigram MLE: P(w|h) = C(h,w) / C(h)

From the table:
- P(I|<s>) = 2/3
- P(love|I) = 1
- P(NLP|love) = 1/2
- P(deep|love) = 1/2
- P(learning|deep) = 1
- P(</s>|NLP) = 1
- P(</s>|learning) = 1/2

**S1:** <s> I love NLP </s>
P(S1) = (2/3)Â·1Â·(1/2)Â·1 = 1/3

**S2:** <s> I love deep learning </s>
P(S2) = (2/3)Â·1Â·(1/2)Â·1Â·(1/2) = 1/6

âœ… **More probable:** S1.

### (B) Zero-probability problem
MLE P(noodle|ate) = 0 because the bigram never appears.
This makes any sentence containing â€œate noodleâ€ have probability 0 (bad for sentence probability and perplexity).

**Add-1 smoothing** (given |V|=10, total after â€œateâ€ = 12):
P_add1(noodle|ate) = (0+1)/(12+10) = 1/22


#Q4) Backoff Model

A backoff model means:

Try the highest-order model first (trigram).

If itâ€™s unseen (probability 0), back off to a lower-order model (bigram).

If thatâ€™s also unseen, back off again (unigram).

âœ… Key Formulas
1) Trigram MLE
ğ‘ƒ
(
ğ‘¤
ğ‘–
âˆ£
ğ‘¤
ğ‘–
âˆ’
2
,
ğ‘¤
ğ‘–
âˆ’
1
)
=
ğ¶
(
ğ‘¤
ğ‘–
âˆ’
2
,
ğ‘¤
ğ‘–
âˆ’
1
,
ğ‘¤
ğ‘–
)
ğ¶
(
ğ‘¤
ğ‘–
âˆ’
2
,
ğ‘¤
ğ‘–
âˆ’
1
)
P(w
i
	â€‹

âˆ£w
iâˆ’2
	â€‹

,w
iâˆ’1
	â€‹

)=
C(w
iâˆ’2
	â€‹

,w
iâˆ’1
	â€‹

)
C(w
iâˆ’2
	â€‹

,w
iâˆ’1
	â€‹

,w
i
	â€‹

)
	â€‹

2) Bigram MLE (backoff level 1)
ğ‘ƒ
(
ğ‘¤
ğ‘–
âˆ£
ğ‘¤
ğ‘–
âˆ’
1
)
=
ğ¶
(
ğ‘¤
ğ‘–
âˆ’
1
,
ğ‘¤
ğ‘–
)
ğ¶
(
ğ‘¤
ğ‘–
âˆ’
1
)
P(w
i
	â€‹

âˆ£w
iâˆ’1
	â€‹

)=
C(w
iâˆ’1
	â€‹

)
C(w
iâˆ’1
	â€‹

,w
i
	â€‹

)
	â€‹

3) Unigram MLE (backoff level 2)
ğ‘ƒ
(
ğ‘¤
ğ‘–
)
=
ğ¶
(
ğ‘¤
ğ‘–
)
ğ‘
P(w
i
	â€‹

)=
N
C(w
i
	â€‹

)
	â€‹

âœ… (a) Compute 
ğ‘ƒ
(
cats
âˆ£
ğ¼
,
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
)
P(catsâˆ£I,like)

We first try trigram probability because we have a two-word history (I, like).

Given counts (from the question):

ğ¶
(
ğ¼
,
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
,
ğ‘
ğ‘
ğ‘¡
ğ‘ 
)
=
1
C(I,like,cats)=1

ğ¶
(
ğ¼
,
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
)
=
2
C(I,like)=2

Apply trigram formula:

ğ‘ƒ
(
ğ‘
ğ‘
ğ‘¡
ğ‘ 
âˆ£
ğ¼
,
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
)
=
ğ¶
(
ğ¼
,
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
,
ğ‘
ğ‘
ğ‘¡
ğ‘ 
)
ğ¶
(
ğ¼
,
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
)
=
1
2
=
0.5
P(catsâˆ£I,like)=
C(I,like)
C(I,like,cats)
	â€‹

=
2
1
	â€‹

=0.5

âœ… Answer: 
0.5
0.5
	â€‹


âœ… (b) Compute 
ğ‘ƒ
(
dogs
âˆ£
ğ‘Œ
ğ‘œ
ğ‘¢
,
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
)
P(dogsâˆ£You,like) using trigram â†’ bigram backoff
Step 1 â€” Try trigram first

We check:

ğ¶
(
ğ‘Œ
ğ‘œ
ğ‘¢
,
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
,
ğ‘‘
ğ‘œ
ğ‘”
ğ‘ 
)
C(You,like,dogs)

The trigram (You like dogs) does not appear in the corpus, so:

ğ¶
(
ğ‘Œ
ğ‘œ
ğ‘¢
,
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
,
ğ‘‘
ğ‘œ
ğ‘”
ğ‘ 
)
=
0
â‡’
ğ‘ƒ
(
ğ‘‘
ğ‘œ
ğ‘”
ğ‘ 
âˆ£
ğ‘Œ
ğ‘œ
ğ‘¢
,
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
)
=
0
C(You,like,dogs)=0â‡’P(dogsâˆ£You,like)=0

That means trigram MLE fails (zero probability), so we back off.

Step 2 â€” Back off to bigram

Now we compute using only the most recent word like:

Given counts:

ğ¶
(
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
,
ğ‘‘
ğ‘œ
ğ‘”
ğ‘ 
)
=
1
C(like,dogs)=1

ğ¶
(
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
)
=
3
C(like)=3

Apply bigram formula:

ğ‘ƒ
(
ğ‘‘
ğ‘œ
ğ‘”
ğ‘ 
âˆ£
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
)
=
ğ¶
(
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
,
ğ‘‘
ğ‘œ
ğ‘”
ğ‘ 
)
ğ¶
(
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
)
=
1
3
â‰ˆ
0.333
P(dogsâˆ£like)=
C(like)
C(like,dogs)
	â€‹

=
3
1
	â€‹

â‰ˆ0.333

âœ… Answer (with backoff):

ğ‘ƒ
(
ğ‘‘
ğ‘œ
ğ‘”
ğ‘ 
âˆ£
ğ‘Œ
ğ‘œ
ğ‘¢
,
ğ‘™
ğ‘–
ğ‘˜
ğ‘’
)
â‰ˆ
1
3
P(dogsâˆ£You,like)â‰ˆ
3
1
	â€‹

	â€‹

(c) Why backoff is necessary (important explanation)

In real text data, the number of possible trigrams is huge, so most valid trigrams wonâ€™t appear in a small training set.

Without backoff:

unseen trigram â†’ probability becomes 0

then any sentence containing that trigram gets total probability 0

the model cannot compare sentences correctly (everything can collapse to zero)

With backoff:

the model still assigns a reasonable probability using bigrams or unigrams

results become more stable and realistic under sparse data conditions
	â€‹


## Q5) Multi-class Metrics
Confusion matrix (System rows Ã— Gold columns):

|        | Cat | Dog | Rabbit |
|--------|-----|-----|--------|
| Cat    | 5   | 10  | 5      |
| Dog    | 15  | 20  | 10     |
| Rabbit | 0   | 15  | 10     |

Row sums (pred): Cat=20, Dog=45, Rabbit=25
Col sums (gold): Cat=20, Dog=45, Rabbit=25
TP: Cat=5, Dog=20, Rabbit=10

Per-class:
- Cat: Precision=5/20=0.25, Recall=5/20=0.25
- Dog: Precision=20/45=0.4444, Recall=20/45=0.4444
- Rabbit: Precision=10/25=0.40, Recall=10/25=0.40

Macro Precision/Recall â‰ˆ (0.25+0.4444+0.40)/3 â‰ˆ 0.3648
Micro Precision/Recall = (5+20+10)/90 = 35/90 â‰ˆ 0.3889

 Code prints all metrics clearly.

---

# Part II â€” Programming

## Q1) Bigram Language Model (MLE)
- Builds unigram + bigram counts from the 3 training sentences.
- Computes sentence probabilities for:
  - <s> I love NLP </s>
  - <s> I love deep learning </s>
- Prints which sentence is preferred (higher probability).

## How to Run
1. Open the notebook: **CS5760_HW2_Shaik_Karishma_700768890.ipynb**
2. Run all cells top-to-bottom.
3. Outputs will print in the notebook.
