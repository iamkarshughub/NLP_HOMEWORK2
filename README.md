# CS5760 â€“ Natural Language Processing
## Homework 2

**Student Name:** Shaik Karishma  
**Student ID:** 700768890



---

# Part I â€” Writing / Calculations 

## Q1) Worked Example: Classify â€œpredictable no funâ€
Naive Bayes score:

Score(c) = P(c) Ã— Î  P(w_i | c)

For **â€œpredictable no funâ€**:
- Score(pos) = P(pos)Â·P(predictable|pos)Â·P(no|pos)Â·P(fun|pos)
- Score(neg) = P(neg)Â·P(predictable|neg)Â·P(no|neg)Â·P(fun|neg)

 Pick the class with the larger score. (Notebook computes both scores once you plug in the given likelihoods from the slide/Q2.)

#Q2) Harms of Classification

(a) Representational harm
Representational harm happens when a system reinforces stereotypes or unfair associations about a group through how it labels/represents them. In the Kiritchenko & Mohammad (2018) type of setup, this harm is shown when model outputs (e.g., emotion/association predictions) systematically differ across demographic identity terms, reflecting biased language patterns in training data and causing unfair representation.

(b) One risk of censorship in toxicity classification
A key risk is over-blocking or silencing: systems may label non-toxic content as toxic (especially when it contains identity terms or reclaimed language), causing communities to get unfairly moderated and reducing legitimate speech.

(c) Why worse on African American English / Indian English
Models often perform worse because training/test data is not balanced across dialects. If a model is mostly trained on â€œstandardâ€ English, dialect grammar/spelling/phrasing looks â€œunusualâ€ to the model, increasing false positives/negatives.

Q3) Bigram Probabilities and Zero-Probability Problem 


A) Sentence probability (MLE)

MLE bigram formula:

ğ‘ƒ
(
ğ‘¤
ğ‘–
âˆ£
ğ‘¤
ğ‘–
âˆ’
1
)
=
ğ¶
(
ğ‘¤
ğ‘–
âˆ’
1
,
ğ‘¤
ğ‘–
)
ğ¶
(
ğ‘¤
ğ‘–
âˆ’
1
)
P(w
i
	â€‹

âˆ£w
iâˆ’1
	â€‹

)=
C(w
iâˆ’1
	â€‹

)
C(w
iâˆ’1
	â€‹

,w
i
	â€‹

)
	â€‹


S1: 
âŸ¨
ğ‘ 
âŸ©
 
ğ¼
 
ğ‘™
ğ‘œ
ğ‘£
ğ‘’
 
ğ‘
ğ¿
ğ‘ƒ
 
âŸ¨
/
ğ‘ 
âŸ©
âŸ¨sâŸ© I love NLP âŸ¨/sâŸ©

ğ‘ƒ
(
ğ¼
âˆ£
âŸ¨
ğ‘ 
âŸ©
)
=
2
/
3
P(Iâˆ£âŸ¨sâŸ©)=2/3

ğ‘ƒ
(
ğ‘™
ğ‘œ
ğ‘£
ğ‘’
âˆ£
ğ¼
)
=
2
/
2
=
1
P(loveâˆ£I)=2/2=1

ğ‘ƒ
(
ğ‘
ğ¿
ğ‘ƒ
âˆ£
ğ‘™
ğ‘œ
ğ‘£
ğ‘’
)
=
1
/
2
P(NLPâˆ£love)=1/2

ğ‘ƒ
(
âŸ¨
/
ğ‘ 
âŸ©
âˆ£
ğ‘
ğ¿
ğ‘ƒ
)
=
1
/
1
=
1
P(âŸ¨/sâŸ©âˆ£NLP)=1/1=1

ğ‘ƒ
(
ğ‘†
1
)
=
2
3
â‹…
1
â‹…
1
2
â‹…
1
=
1
3
P(S1)=
3
2
	â€‹

â‹…1â‹…
2
1
	â€‹

â‹…1=
3
1
	â€‹


S2: 
âŸ¨
ğ‘ 
âŸ©
 
ğ¼
 
ğ‘™
ğ‘œ
ğ‘£
ğ‘’
 
ğ‘‘
ğ‘’
ğ‘’
ğ‘
 
ğ‘™
ğ‘’
ğ‘
ğ‘Ÿ
ğ‘›
ğ‘–
ğ‘›
ğ‘”
 
âŸ¨
/
ğ‘ 
âŸ©
âŸ¨sâŸ© I love deep learning âŸ¨/sâŸ©

ğ‘ƒ
(
ğ¼
âˆ£
âŸ¨
ğ‘ 
âŸ©
)
=
2
/
3
P(Iâˆ£âŸ¨sâŸ©)=2/3

ğ‘ƒ
(
ğ‘™
ğ‘œ
ğ‘£
ğ‘’
âˆ£
ğ¼
)
=
1
P(loveâˆ£I)=1

ğ‘ƒ
(
ğ‘‘
ğ‘’
ğ‘’
ğ‘
âˆ£
ğ‘™
ğ‘œ
ğ‘£
ğ‘’
)
=
1
/
2
P(deepâˆ£love)=1/2

ğ‘ƒ
(
ğ‘™
ğ‘’
ğ‘
ğ‘Ÿ
ğ‘›
ğ‘–
ğ‘›
ğ‘”
âˆ£
ğ‘‘
ğ‘’
ğ‘’
ğ‘
)
=
2
/
2
=
1
P(learningâˆ£deep)=2/2=1

ğ‘ƒ
(
âŸ¨
/
ğ‘ 
âŸ©
âˆ£
ğ‘™
ğ‘’
ğ‘
ğ‘Ÿ
ğ‘›
ğ‘–
ğ‘›
ğ‘”
)
=
1
/
2
P(âŸ¨/sâŸ©âˆ£learning)=1/2

ğ‘ƒ
(
ğ‘†
2
)
=
2
3
â‹…
1
â‹…
1
2
â‹…
1
â‹…
1
2
=
1
6
P(S2)=
3
2
	â€‹

â‹…1â‹…
2
1
	â€‹

â‹…1â‹…
2
1
	â€‹

=
6
1
	â€‹


âœ… Model prefers S1 because 
1
/
3
>
1
/
6
1/3>1/6.

B) Zero-probability problem

MLE:

ğ‘ƒ
(
ğ‘›
ğ‘œ
ğ‘œ
ğ‘‘
ğ‘™
ğ‘’
âˆ£
ğ‘
ğ‘¡
ğ‘’
)
=
0
12
=
0
P(noodleâˆ£ate)=
12
0
	â€‹

=0

This is a problem because if any one bigram probability is 0, then the entire sentence probability becomes 0, which breaks probability comparisons and makes perplexity blow up / become undefined.

C) Laplace smoothing (Add-1)

Given: vocab size 
ğ‘‰
=
10
V=10, total count after â€œateâ€ is 12, and count(ate,noodle)=0:

ğ‘ƒ
(
ğ‘›
ğ‘œ
ğ‘œ
ğ‘‘
ğ‘™
ğ‘’
âˆ£
ğ‘
ğ‘¡
ğ‘’
)
=
0
+
1
12
+
10
=
1
22
P(noodleâˆ£ate)=
12+10
0+1
	â€‹

=
22
1
	â€‹


## Q4) Backoff Model
Counts: I like = 2, You like = 1, like cats = 2, like dogs = 1

1) P(cats|I,like) = C(I like cats) / C(I like) = 1/2
2) P(dogs|You,like): trigram unseen â†’ backoff to bigram
   P(dogs|like) = C(like dogs) / C(like) = 1/(2+1) = 1/3
3) Backoff is needed because small corpora have many unseen trigrams; backoff avoids zero probabilities.

## Q5) Multi-class Metrics
Confusion matrix (System rows Ã— Gold columns):

|        | Cat | Dog | Rabbit |
|--------|-----|-----|--------|
| Cat    | 5   | 10  | 5      |
| Dog    | 15  | 20  | 10     |
| Rabbit | 0   | 15  | 10     |

Row sums (pred): Cat=20, Dog=45, Rabbit=25
Col sums (gold): Cat=20, Dog=45, Rabbit=25
TP: Cat=5, Dog=20, Rabbit=10

Per-class:
- Cat: Precision=5/20=0.25, Recall=5/20=0.25
- Dog: Precision=20/45=0.4444, Recall=20/45=0.4444
- Rabbit: Precision=10/25=0.40, Recall=10/25=0.40

Macro Precision/Recall â‰ˆ (0.25+0.4444+0.40)/3 â‰ˆ 0.3648
Micro Precision/Recall = (5+20+10)/90 = 35/90 â‰ˆ 0.3889

 Code prints all metrics clearly.

---

# Part II â€” Programming

## Q1) Bigram Language Model (MLE)
- Builds unigram + bigram counts from the 3 training sentences.
- Computes sentence probabilities for:
  - <s> I love NLP </s>
  - <s> I love deep learning </s>
- Prints which sentence is preferred (higher probability).

## How to Run
1. Open the notebook: **CS5760_HW2_Shaik_Karishma_700768890.ipynb**
2. Run all cells top-to-bottom.
3. Outputs will print in the notebook.
