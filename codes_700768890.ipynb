{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CS5760 — Homework 2\n",
        "\n",
        "**Student Name:** Shaik Karishma  \n",
        "**Student ID:** 700768890\n",
        "\n",
        "Run cells top-to-bottom."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q5 — Confusion Matrix Metrics (Programming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix: system rows x gold columns\n",
        "labels = [\"Cat\", \"Dog\", \"Rabbit\"]\n",
        "cm = [\n",
        "    [5, 10, 5],   # predicted Cat\n",
        "    [15, 20, 10], # predicted Dog\n",
        "    [0, 15, 10],  # predicted Rabbit\n",
        "]\n",
        "\n",
        "def compute_metrics(cm, labels):\n",
        "    n = len(labels)\n",
        "    row_sums = [sum(cm[i]) for i in range(n)]  # predicted totals\n",
        "    col_sums = [sum(cm[i][j] for i in range(n)) for j in range(n)]  # gold totals\n",
        "\n",
        "    per_class = {}\n",
        "    for k, lab in enumerate(labels):\n",
        "        tp = cm[k][k]\n",
        "        precision = tp / row_sums[k] if row_sums[k] else 0.0\n",
        "        recall = tp / col_sums[k] if col_sums[k] else 0.0\n",
        "        per_class[lab] = (precision, recall, tp, row_sums[k], col_sums[k])\n",
        "\n",
        "    macro_p = sum(per_class[lab][0] for lab in labels) / n\n",
        "    macro_r = sum(per_class[lab][1] for lab in labels) / n\n",
        "\n",
        "    total_correct = sum(cm[i][i] for i in range(n))\n",
        "    total = sum(sum(row) for row in cm)\n",
        "    micro_p = total_correct / total\n",
        "    micro_r = micro_p  # multiclass single-label\n",
        "\n",
        "    return per_class, (macro_p, macro_r), (micro_p, micro_r)\n",
        "\n",
        "per_class, (macro_p, macro_r), (micro_p, micro_r) = compute_metrics(cm, labels)\n",
        "\n",
        "print(\"Per-class metrics:\")\n",
        "for lab in labels:\n",
        "    p, r, tp, pred_total, gold_total = per_class[lab]\n",
        "    print(f\"  {lab}: TP={tp}, Pred={pred_total}, Gold={gold_total}, Precision={p:.4f}, Recall={r:.4f}\")\n",
        "\n",
        "print(\"\\nMacro Precision:\", round(macro_p, 4))\n",
        "print(\"Macro Recall:\", round(macro_r, 4))\n",
        "print(\"Micro Precision:\", round(micro_p, 4))\n",
        "print(\"Micro Recall:\", round(micro_r, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part II Q1 — Bigram Language Model (MLE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training corpus (as given)\n",
        "corpus_sents = [\n",
        "    [\"<s>\", \"I\", \"love\", \"NLP\", \"</s>\"],\n",
        "    [\"<s>\", \"I\", \"love\", \"deep\", \"learning\", \"</s>\"],\n",
        "    [\"<s>\", \"deep\", \"learning\", \"is\", \"fun\", \"</s>\"],\n",
        "]\n",
        "\n",
        "# Unigram and bigram counts\n",
        "unigram = Counter()\n",
        "bigram = Counter()\n",
        "\n",
        "for sent in corpus_sents:\n",
        "    unigram.update(sent)\n",
        "    bigram.update(zip(sent[:-1], sent[1:]))\n",
        "\n",
        "def p_bigram_mle(h, w):\n",
        "    denom = unigram[h]\n",
        "    return (bigram[(h, w)] / denom) if denom else 0.0\n",
        "\n",
        "def sentence_probability(tokens):\n",
        "    prob = 1.0\n",
        "    for h, w in zip(tokens[:-1], tokens[1:]):\n",
        "        prob *= p_bigram_mle(h, w)\n",
        "    return prob\n",
        "\n",
        "S1 = [\"<s>\", \"I\", \"love\", \"NLP\", \"</s>\"]\n",
        "S2 = [\"<s>\", \"I\", \"love\", \"deep\", \"learning\", \"</s>\"]\n",
        "\n",
        "p1 = sentence_probability(S1)\n",
        "p2 = sentence_probability(S2)\n",
        "\n",
        "print(\"P(S1) =\", p1)\n",
        "print(\"P(S2) =\", p2)\n",
        "\n",
        "if p1 > p2:\n",
        "    print(\"Preferred: S1 (<s> I love NLP </s>) because it has higher probability.\")\n",
        "elif p2 > p1:\n",
        "    print(\"Preferred: S2 (<s> I love deep learning </s>) because it has higher probability.\")\n",
        "else:\n",
        "    print(\"Both sentences have equal probability.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part I Q3/Q4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bigram table from HW2 Part I Q3\n",
        "counts = {\n",
        "    \"<s>\": {\"I\": 2, \"deep\": 1},\n",
        "    \"I\": {\"love\": 2},\n",
        "    \"love\": {\"NLP\": 1, \"deep\": 1},\n",
        "    \"deep\": {\"learning\": 2},\n",
        "    \"learning\": {\"</s>\": 1, \"is\": 1},\n",
        "    \"NLP\": {\"</s>\": 1},\n",
        "    \"is\": {\"fun\": 1},\n",
        "    \"fun\": {\"</s>\": 1},\n",
        "    \"ate\": {\"lunch\": 6, \"dinner\": 3, \"a\": 2, \"the\": 1},\n",
        "}\n",
        "\n",
        "def total_after(h):\n",
        "    return sum(counts.get(h, {}).values())\n",
        "\n",
        "def mle(h, w):\n",
        "    denom = total_after(h)\n",
        "    return (counts.get(h, {}).get(w, 0) / denom) if denom else 0.0\n",
        "\n",
        "def sent_prob(tokens):\n",
        "    p = 1.0\n",
        "    for h, w in zip(tokens[:-1], tokens[1:]):\n",
        "        p *= mle(h, w)\n",
        "    return p\n",
        "\n",
        "S1 = [\"<s>\", \"I\", \"love\", \"NLP\", \"</s>\"]\n",
        "S2 = [\"<s>\", \"I\", \"love\", \"deep\", \"learning\", \"</s>\"]\n",
        "print(\"P(S1) =\", sent_prob(S1))\n",
        "print(\"P(S2) =\", sent_prob(S2))\n",
        "print(\"Preferred:\", \"S1\" if sent_prob(S1) > sent_prob(S2) else \"S2\")\n",
        "\n",
        "print(\"MLE P(noodle|ate) =\", mle(\"ate\", \"noodle\"))\n",
        "\n",
        "# Add-1 smoothing (given |V|=10, total after ate=12)\n",
        "V = 10\n",
        "N_after_ate = 12\n",
        "print(\"Add-1 P(noodle|ate) =\", (0 + 1) / (N_after_ate + V))\n",
        "\n",
        "# Backoff checks from Q4\n",
        "print(\"P(cats|I,like) =\", 1/2)\n",
        "print(\"Backoff P(dogs|like) =\", 1/3)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CS5760_HW2_Shaik_Karishma_700768890.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
